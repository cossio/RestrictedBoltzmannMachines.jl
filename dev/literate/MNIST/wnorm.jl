#=
# Weight normalization

The authors of <https://arxiv.org/abs/1602.07868> introduce weight normalization to boost
learning.
Let's try it here.

Preliminaries:
=#

using CairoMakie, Statistics
import MLDatasets, Flux
import RestrictedBoltzmannMachines as RBMs
nothing #hide

#

function imggrid(A::AbstractArray{<:Any,4})
    return reshape(permutedims(A, (1,3,2,4)), size(A,1)*size(A,3), size(A,2)*size(A,4))
end

# Load data

train_x, train_y = MLDatasets.MNIST.traindata()
tests_x, tests_y = MLDatasets.MNIST.testdata()
selected_digits = (0, 1)
train_x = train_x[:, :, train_y .∈ Ref(selected_digits)] .≥ 0.5
tests_x = tests_x[:, :, tests_y .∈ Ref(selected_digits)] .≥ 0.5
train_y = train_y[train_y .∈ Ref(selected_digits)]
tests_y = tests_y[tests_y .∈ Ref(selected_digits)]
train_nsamples = length(train_y)
tests_nsamples = length(tests_y)
(train_nsamples, tests_nsamples)

#

Float = Float32
train_x = Array{Float}(train_x)
tests_x = Array{Float}(tests_x)
nothing #hide

#=
Init RBM and train.
=#

rbm = RBMs.RBM(
    RBMs.Binary(Float,28,28),
    RBMs.Binary(Float,200),
    randn(Float,28,28,200)/28
)
RBMs.initialize!(rbm, train_x)
wn = RBMs.WeightNorm(rbm)
@time history_wnorm = RBMs.pcd!(
    rbm, wn, train_x;
    epochs=200, batchsize=256, steps=1, optimizer=Flux.ADAM()
)
nothing #hide

#=
Let's see what the learning curves look like.
=#

fig = Figure(resolution=(800, 300))
ax = Axis(fig[1,1])
lines!(ax, get(history_wnorm, :lpl)..., label="w. norm.")
axislegend(ax, position=:rb)
fig

#=
Let's look at some samples generated by this RBM.
=#

nrows, ncols = 10, 15
fantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]
fantasy_x .= RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)
fig = Figure(resolution=(40ncols, 40nrows))
ax = Axis(fig[1,1], yreversed=true)
image!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))
hidedecorations!(ax)
hidespines!(ax)
fig
