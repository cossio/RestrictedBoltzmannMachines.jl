var documenterSearchIndex = {"docs":
[{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/lr_decay.jl\"","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Effect of decaying the learning rate during training to achieve convergence.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"using CairoMakie, Statistics, Random, LinearAlgebra\nimport Flux, MLDatasets, ValueHistories\nimport RestrictedBoltzmannMachines as RBMs","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Load MNIST dataset.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Float = Float32;\ntrain_x, train_y = MLDatasets.MNIST.traindata();\ntests_x, tests_y = MLDatasets.MNIST.testdata();\ntrain_x = Float.(train_x .≥ 0.5);\ntests_x = Float.(tests_x .≥ 0.5);\nnothing #hide","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"select only 0s digits","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"train_x = train_x[:,:, train_y .== 0]\ntests_x = tests_x[:,:, tests_y .== 0]\ntrain_y = train_y[train_y .== 0]\ntests_y = tests_y[tests_y .== 0]\ntrain_nsamples = length(train_y)\ntests_nsamples = length(tests_y)\nnothing #hide","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Consider first an RBM that we train without decaying the learning rate. We will train this machine for 300 epochs.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"rbm = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,100), zeros(Float,28,28,100));\nRBMs.initialize!(rbm, train_x);\nopt = Flux.ADAM(0.001f0, (0.9f0, 0.999f0))\nhistory = ValueHistories.MVHistory()\nRBMs.pcd!(rbm, train_x; history=history,\n    epochs=300, batchsize=128, steps=1, optimizer=opt\n)\nnothing #hide","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Now train an RBM with 200 normal epochs, followed by 100 epochs where the learning-rate is cut in half every 10 epochs.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"rbm_decay = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,100), zeros(Float,28,28,100));\nRBMs.initialize!(rbm_decay, train_x);\nopt = Flux.ADAM(0.001f0, (0.9f0, 0.999f0))\nhistory_decay = ValueHistories.MVHistory()\nRBMs.pcd!(rbm_decay, train_x; history=history_decay,\n    epochs=200, batchsize=128, steps=1,\n    optimizer=opt\n)\nprintln(\"*** decaying learning rate ***\")\nfor meta_epoch = 1:10\n    opt.eta /= 2\n    RBMs.pcd!(rbm_decay, train_x; history=history_decay,\n        epochs=10, batchsize=128, steps=1,\n        optimizer=opt\n    )\nend\nnothing #hide","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Compare the results","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"fig = Figure(resolution=(600,400))\nax = Axis(fig[1,1])\nlines!(ax, get(history, :lpl)..., label=\"normal\")\nlines!(ax, get(history_decay, :lpl)..., label=\"decay\")\naxislegend(ax, position=:rb)\nfig","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Check convergence by computing the moment-matching conditions. First generate MC data from the RBMs.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"samples_v = RBMs.sample_v_from_v(rbm, tests_x; steps=1000)\nsamples_v_decay = RBMs.sample_v_from_v(rbm_decay, tests_x; steps=1000)\nnothing #hide","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Now make the plots","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"For the RBM with constant learning-rate.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"fig = Figure(resolution=(900, 300))\nax = Axis(fig[1,1])\nheatmap!(ax, mean(train_x, dims=3)[:,:,1])\nhidedecorations!(ax)\nax = Axis(fig[1,2])\nheatmap!(ax, mean(tests_x, dims=3)[:,:,1])\nhidedecorations!(ax)\nax = Axis(fig[1,3])\nheatmap!(ax, mean(samples_v, dims=3)[:,:,1])\nhidedecorations!(ax)\nfig","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Moment matching conditions, first for RBM with constant learning rate","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"h_data = RBMs.mean_h_from_v(rbm, train_x);\nh_model = RBMs.mean_h_from_v(rbm, samples_v);\n\nfig = Figure(resolution=(900, 300))\n\nax = Axis(fig[1,1], xlabel=L\"\\langle v_i \\rangle_\\mathrm{data}\", ylabel=L\"\\langle v_i \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax, vec(mean(train_x; dims=3)), vec(mean(samples_v; dims=3)))\nabline!(ax, 0, 1; color=:red)\n\nax = Axis(fig[1,2], xlabel=L\"\\langle h_\\mu \\rangle_\\mathrm{data}\", ylabel=L\"\\langle h_\\mu \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax, vec(mean(h_data; dims=2)), vec(mean(h_model; dims=2)))\nabline!(ax, 0, 1; color=:red)\n\nax = Axis(fig[1,3], xlabel=L\"\\langle v_i h_\\mu \\rangle_\\mathrm{data}\", ylabel=L\"\\langle v_i h_\\mu \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax,\n    vec([dot(train_x[i,j,:], h_data[μ,:]) / size(train_x,3) for i=1:28, j=1:28, μ=1:100]),\n    vec([dot(samples_v[i,j,:], h_model[μ,:]) / size(samples_v,3) for i=1:28, j=1:28, μ=1:100])\n)\nabline!(ax, 0, 1; color=:red)\n\nfig","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"For the RBM with decaying learning-rate.","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"fig = Figure(resolution=(900, 300))\nax = Axis(fig[1,1])\nheatmap!(ax, mean(train_x, dims=3)[:,:,1])\nhidedecorations!(ax)\nax = Axis(fig[1,2])\nheatmap!(ax, mean(tests_x, dims=3)[:,:,1])\nhidedecorations!(ax)\nax = Axis(fig[1,3])\nheatmap!(ax, mean(samples_v_decay, dims=3)[:,:,1])\nhidedecorations!(ax)\nfig","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"Moment matching conditions","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"h_data = RBMs.mean_h_from_v(rbm_decay, train_x);\nh_model = RBMs.mean_h_from_v(rbm_decay, samples_v_decay);\n\nfig = Figure(resolution=(900, 300))\n\nax = Axis(fig[1,1], xlabel=L\"\\langle v_i \\rangle_\\mathrm{data}\", ylabel=L\"\\langle v_i \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax, vec(mean(train_x; dims=3)), vec(mean(samples_v_decay; dims=3)))\nabline!(ax, 0, 1; color=:red)\n\nax = Axis(fig[1,2], xlabel=L\"\\langle h_\\mu \\rangle_\\mathrm{data}\", ylabel=L\"\\langle h_\\mu \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax, vec(mean(h_data; dims=2)), vec(mean(h_model; dims=2)))\nabline!(ax, 0, 1; color=:red)\n\nax = Axis(fig[1,3], xlabel=L\"\\langle v_i h_\\mu \\rangle_\\mathrm{data}\", ylabel=L\"\\langle v_i h_\\mu \\rangle_\\mathrm{model}\", limits=(0,1,0,1))\nscatter!(ax,\n    vec([dot(train_x[i,j,:], h_data[μ,:]) / size(train_x,3) for i=1:28, j=1:28, μ=1:100]),\n    vec([dot(samples_v_decay[i,j,:], h_model[μ,:]) / size(samples_v_decay,3) for i=1:28, j=1:28, μ=1:100])\n)\nabline!(ax, 0, 1; color=:red)\n\nfig","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"","category":"page"},{"location":"literate/lr_decay/","page":"Learning rate decay","title":"Learning rate decay","text":"This page was generated using Literate.jl.","category":"page"},{"location":"literate/MNIST/white/","page":"Whitening the data","title":"Whitening the data","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/MNIST/white.jl\"","category":"page"},{"location":"literate/MNIST/white/#Whitening-the-data","page":"Whitening the data","title":"Whitening the data","text":"","category":"section"},{"location":"literate/MNIST/white/","page":"Whitening the data","title":"Whitening the data","text":"https://www.jmlr.org/beta/papers/v17/14-237.html, http://www.cs.toronto.edu/~tang/papers/RbmZM.pdf, https://doi.org/10.1007/978-3-642-35289-8_3","category":"page"},{"location":"literate/MNIST/white/","page":"Whitening the data","title":"Whitening the data","text":"","category":"page"},{"location":"literate/MNIST/white/","page":"Whitening the data","title":"Whitening the data","text":"This page was generated using Literate.jl.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/layers/Gaussian.jl\"","category":"page"},{"location":"literate/layers/Gaussian/#Gaussian-layer","page":"Gaussian","title":"Gaussian layer","text":"","category":"section"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"In the following example we look at what the Gaussian layer hidden units look like, for different parameter values.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"First load some packages.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"import RestrictedBoltzmannMachines as RBMs\nusing CairoMakie, Statistics\nnothing #hide","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"Now initialize our Gaussian layer, with unit parameters spanning an interesting range.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"θs = [-5; 5]\nγs = [1; 2]\nlayer = RBMs.Gaussian([θ for θ in θs, γ in γs], [γ for θ in θs, γ in γs])\nnothing #hide","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"Now we sample our layer to collect some data.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"data = RBMs.transfer_sample(layer, zeros(size(layer)..., 10^6))\nnothing #hide","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"Let's plot the resulting histogram of the activations of each unit. We also overlay the analytical PDF.","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"fig = Figure(resolution=(700,500))\nax = Axis(fig[1,1])\nxs = repeat(reshape(range(minimum(data), maximum(data), 100), 1, 1, 100), size(layer)...)\nps = exp.(RBMs.free_energies(layer) .- RBMs.energies(layer, xs))\nfor (iθ, θ) in enumerate(θs), (iγ, γ) in enumerate(γs)\n    hist!(ax, data[iθ, iγ, :], normalization=:pdf, label=\"θ=$θ, γ=$γ\")\n    lines!(xs[iθ, iγ, :], ps[iθ, iγ, :], linewidth=2)\nend\naxislegend(ax)\nfig","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"","category":"page"},{"location":"literate/layers/Gaussian/","page":"Gaussian","title":"Gaussian","text":"This page was generated using Literate.jl.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/layers/dReLU.jl\"","category":"page"},{"location":"literate/layers/dReLU/#dReLU-layer","page":"dReLU","title":"dReLU layer","text":"","category":"section"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"In this example we look at what the dReLU layer hidden units look like, for different parameter values.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"First load some packages.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"import RestrictedBoltzmannMachines as RBMs\nusing CairoMakie, Statistics\nnothing #hide","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"Now initialize our dReLU layer, with unit parameters spanning an interesting range.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"θps = [-3.0; 3.0]\nθns = [-3.0; 3.0]\nγps = [0.5; 1.0]\nγns = [0.5; 1.0]\nlayer = RBMs.dReLU(\n    [θp for θp in θps, θn in θns, γp in γps, γn in γns],\n    [θn for θp in θps, θn in θns, γp in γps, γn in γns],\n    [γp for θp in θps, θn in θns, γp in γps, γn in γns],\n    [γn for θp in θps, θn in θns, γp in γps, γn in γns]\n)\nnothing #hide","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"Now we sample our layer to collect some data.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"data = RBMs.transfer_sample(layer, zeros(size(layer)..., 10^6))\nnothing #hide","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"Let's plot the resulting histogram of the activations of each unit. We also overlay the analytical PDF.","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"fig = Figure(resolution=(1000, 700))\nxs = repeat(reshape(range(minimum(data), maximum(data), 100), 1,1,1,1,100), size(layer)...)\nps = exp.(RBMs.free_energies(layer) .- RBMs.energies(layer, xs))\nfor (iθp, θp) in enumerate(θps), (iθn, θn) in enumerate(θns)\n    ax = Axis(fig[iθp,iθn], title=\"θp=$θp, θn=$θn\", xlabel=\"h\", ylabel=\"P(h)\")\n    for (iγp, γp) in enumerate(γps), (iγn, γn) in enumerate(γns)\n        hist!(ax, data[iθp, iθn, iγp, iγn, :], normalization=:pdf, bins=30, label=\"γp=$γp, γn=$γn\")\n        lines!(ax, xs[iθp, iθn, iγp, iγn, :], ps[iθp, iθn, iγp, iγn, :], linewidth=2)\n    end\n    if iθp == iθn == 1\n        axislegend(ax)\n    end\nend\nfig","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"","category":"page"},{"location":"literate/layers/dReLU/","page":"dReLU","title":"dReLU","text":"This page was generated using Literate.jl.","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/MNIST/wnorm.jl\"","category":"page"},{"location":"literate/MNIST/wnorm/#Weight-normalization","page":"Weight normalization","title":"Weight normalization","text":"","category":"section"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"The authors of https://arxiv.org/abs/1602.07868 introduce weight normalization to boost learning. Let's try it here.","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Preliminaries:","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"using MKL, CairoMakie, Statistics\nimport MLDatasets, Flux\nimport RestrictedBoltzmannMachines as RBMs\nnothing #hide","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"function imggrid(A::AbstractArray{<:Any,4})\n    return reshape(permutedims(A, (1,3,2,4)), size(A,1)*size(A,3), size(A,2)*size(A,4))\nend","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Load data","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"train_x, train_y = MLDatasets.MNIST.traindata()\ntests_x, tests_y = MLDatasets.MNIST.testdata()\nselected_digits = (0, 1)\ntrain_x = train_x[:, :, train_y .∈ Ref(selected_digits)] .≥ 0.5\ntests_x = tests_x[:, :, tests_y .∈ Ref(selected_digits)] .≥ 0.5\ntrain_y = train_y[train_y .∈ Ref(selected_digits)]\ntests_y = tests_y[tests_y .∈ Ref(selected_digits)]\ntrain_nsamples = length(train_y)\ntests_nsamples = length(tests_y)\n(train_nsamples, tests_nsamples)","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Float = Float32\ntrain_x = Array{Float}(train_x)\ntests_x = Array{Float}(tests_x)\nnothing #hide","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Init RBM and train.","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"rbm = RBMs.RBM(\n    RBMs.Binary(Float,28,28),\n    RBMs.Binary(Float,200),\n    randn(Float,28,28,200)/28\n)\nRBMs.initialize!(rbm, train_x)\nwn = RBMs.WeightNorm(rbm)\n@time history_wnorm = RBMs.pcd!(\n    rbm, wn, train_x;\n    epochs=200, batchsize=256, steps=1, optimizer=Flux.ADAM()\n)\nnothing #hide","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Let's see what the learning curves look like.","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"fig = Figure(resolution=(800, 300))\nax = Axis(fig[1,1])\nlines!(ax, get(history_wnorm, :lpl)..., label=\"w. norm.\")\naxislegend(ax, position=:rb)\nfig","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"Let's look at some samples generated by this RBM.","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"nrows, ncols = 10, 15\nfantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]\nfantasy_x .= RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"","category":"page"},{"location":"literate/MNIST/wnorm/","page":"Weight normalization","title":"Weight normalization","text":"This page was generated using Literate.jl.","category":"page"},{"location":"reference/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [RBMs]","category":"page"},{"location":"reference/#RestrictedBoltzmannMachines.Binary","page":"Reference","title":"RestrictedBoltzmannMachines.Binary","text":"Binary(θ)\n\nBinary layer, with external fields θ.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.Gaussian","page":"Reference","title":"RestrictedBoltzmannMachines.Gaussian","text":"Gaussian(θ, γ)\n\nGaussian layer, with location parameters θ and scale parameters γ.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.Potts","page":"Reference","title":"RestrictedBoltzmannMachines.Potts","text":"Potts(θ)\n\nPotts layer, with external fields θ. Encodes categorical variables as one-hot vectors. The number of classes is the size of the first dimension.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.RBM","page":"Reference","title":"RestrictedBoltzmannMachines.RBM","text":"RBM\n\nRepresents a restricted Boltzmann Machine.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.ReLU","page":"Reference","title":"RestrictedBoltzmannMachines.ReLU","text":"ReLU(θ, γ)\n\nReLU layer, with location parameters θ and scale parameters γ.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.Spin","page":"Reference","title":"RestrictedBoltzmannMachines.Spin","text":"Spin(θ)\n\nSpin layer, with external fields θ. The energy of a layer with units s_i is given by:\n\nE = -sum_i theta_i s_i\n\nwhere each spin s_i takes values pm 1.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.StdGauss","page":"Reference","title":"RestrictedBoltzmannMachines.StdGauss","text":"StdGauss\n\nStdGauss layer, a standardized (zero mean, unit variances) Gaussian layer.\n\n\n\n\n\n","category":"type"},{"location":"reference/#RestrictedBoltzmannMachines.WeightNorm-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.WeightNorm","text":"    WeightNorm(rbm, λ = 1)\n\nReturns a re-parameterization of rbm.w, defined by:\n\nmathbfw_mu = g_mu^2 fracmathbfv_mumathbfv_mu\n\nwhere the parameter g_mu^2 encodes the norm of the weight pattern attached to hidden unit mu, while mathbfv_mu encodes its direction.\n\nThe constructor WeightNorm(rbm, λ) initializes g_mu^2 and mathbfv_mu assuming that the vectors mathbfv_mu have norms λ. More precisely, we assume that:\n\nλ = sqrt.(sum(abs2, v; dims=1:ndims(rbm.visible)))\n\nIf not provided λ defaults to ones.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.BinaryRBM","page":"Reference","title":"RestrictedBoltzmannMachines.BinaryRBM","text":"BinaryRBM(a, b, w)\nBinaryRBM(N, M)\n\nConstruct an RBM with binary visible and hidden units, which has an energy function:\n\nE(v h) = -av - bh - vwh\n\nEquivalent to RBM(Binary(a), Binary(b), w).\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.HopfieldRBM-NTuple{4, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.HopfieldRBM","text":"HopfieldRBM(g, θ, γ, w)\nHopfieldRBM(g, w)\n\nConstruct an RBM with spin visible units and Gaussian hidden units. If not given, θ = 0 and γ = 1 by default.\n\nE(v h) = -gv - θh + sum_mu fracγ_mu2 h_mu^2 - vwh\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.L1L2-Tuple{RestrictedBoltzmannMachines.RBM}","page":"Reference","title":"RestrictedBoltzmannMachines.L1L2","text":"L1L2(rbm)\n\nL1/L2 squared norm of rbm.w. Visible unit dimensions are reduced with L1 norm, while hidden unit dimensions are reduced with L2 norm. Note that no square root is taken.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.batchdims-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.batchdims","text":"batchdims(layer, x)\n\nIndices of batch dimensions in x, with respect to layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.batchmean-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.batchmean","text":"batchmean(layer, x; wts = nothing)\n\nMean of x over batch dimensions, with weights wts.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.batchsize-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.batchsize","text":"batchsize(layer, x)\n\nBatch sizes of x, with respect to layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.block_matrix_invert-NTuple{4, AbstractMatrix}","page":"Reference","title":"RestrictedBoltzmannMachines.block_matrix_invert","text":"block_matrix_invert(A, B, C, D)\n\nInversion of a block matrix, using the formula:\n\nbeginbmatrix\n    mathbfA  mathbfB \n    mathbfC  mathbfD\nendbmatrix^-1\n=\nbeginbmatrix\n    left(mathbfA - mathbfB mathbfD^-1 mathbfCright)^-1  mathbf0 \n    mathbf0  left(mathbfD - mathbfC mathbfA^-1 mathbfBright)^-1\nendbmatrix\nbeginbmatrix\n    mathbfI  -mathbfB mathbfD^-1 \n    -mathbfC mathbfA^-1  mathbfI\nendbmatrix\n\nAssumes that A and D are square and invertible.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.block_matrix_logdet-NTuple{4, AbstractMatrix}","page":"Reference","title":"RestrictedBoltzmannMachines.block_matrix_logdet","text":"block_matrix_logdet(A, B, C, D)\n\nLog-determinant of a block matrix using the determinant lemma.\n\ndetleft(\n    beginbmatrix\n        mathbfA  mathbfB \n        mathbfC  mathbfD\n    endbmatrix\nright)\n= det(A) det(D - CA^-1B)\n= det(D) det(A - BD^-1C)\n\nHere we assume that A and D are invertible, and moreover are easy to invert (for example, if they are diagonal). We use this to chose one or the other of the two formulas above.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.broadlike-Tuple{Any, Vararg{Any}}","page":"Reference","title":"RestrictedBoltzmannMachines.broadlike","text":"broadlike(A, B...)\n\nBroadcasts A into the size of A .+ B .+ ... (without actually doing a sum).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.categorical_rand-Tuple{AbstractVector}","page":"Reference","title":"RestrictedBoltzmannMachines.categorical_rand","text":"categorical_rand(ps)\n\nRandomly draw i with probability ps[i]. You must ensure that ps defines a proper probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.categorical_sample-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.categorical_sample","text":"categorical_sample(P)\n\nGiven a probability array P of size (q, *), returns an array C of size (*), such that C[i] ∈ 1:q is a random sample from the categorical distribution P[:,i]. You must ensure that P defines a proper probability distribution.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.categorical_sample_from_logits-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.categorical_sample_from_logits","text":"categorical_sample_from_logits(logits)\n\nGiven a logits array logits of size (q, *) (where q is the number of classes), returns an array X of size (*), such that X[i] is a categorical random sample from the distribution with logits logits[:,i].\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.categorical_sample_from_logits_gumbel-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.categorical_sample_from_logits_gumbel","text":"categorical_sample_from_logits_gumbel(logits)\n\nLike categoricalsamplefrom_logits, but using the Gumbel trick.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.cd!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.cd!","text":"cd!(rbm, data)\n\nTrains the RBM on data using contrastive divergence.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.cdad!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.cdad!","text":"cdad!(rbm, data)\n\nTrains the RBM on data using contrastive divergence. Computes gradients with Zygote.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.collect_states-Tuple{Union{RestrictedBoltzmannMachines.Binary, RestrictedBoltzmannMachines.Potts, RestrictedBoltzmannMachines.Spin}}","page":"Reference","title":"RestrictedBoltzmannMachines.collect_states","text":"collect_states(layer)\n\nReturns an array of all states of layer. Only defined for discrete layers.\n\nwarning: Warning\nUse only for small layers. For large layers, the exponential number of states will not fit in memory.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.colors-Tuple{Union{RestrictedBoltzmannMachines.Binary, RestrictedBoltzmannMachines.Spin}}","page":"Reference","title":"RestrictedBoltzmannMachines.colors","text":"colors(layer)\n\nNumber of possible states of units in discrete layers.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.contrastive_divergence-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.contrastive_divergence","text":"contrastive_divergence(rbm, vd, vm; wd = 1, wm = 1)\n\nContrastive divergence loss. vd is a data sample, and vm are samples from the model.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.default_optimizer-Tuple{Int64, Int64, Int64}","page":"Reference","title":"RestrictedBoltzmannMachines.default_optimizer","text":"default_optimizer(nsamples, batchsize, epochs; opt = ADAM(), decay_after = 0.5)\n\nThe default optimizer decays the learning rate exponentially every epoch, starting after decay_after of training time, with a pre-defined schedule. Based on defaults from https://github.com/jertubiana/PGM.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.energies-Tuple{Union{RestrictedBoltzmannMachines.Binary, RestrictedBoltzmannMachines.Potts, RestrictedBoltzmannMachines.Spin}, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.energies","text":"energies(layer, x)\n\nEnergies of units in layer (not reduced over layer dimensions).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.energy-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.energy","text":"energy(layer, x)\n\nLayer energy, reduced over layer dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.energy-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.energy","text":"energy(rbm, v, h)\n\nEnergy of the rbm in the configuration (v,h).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.fpcd!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.fpcd!","text":"fpcd!(rbm, data)\n\nTrains the RBM on data using Persistent Contrastive divergence, with fast weights. See http://dl.acm.org/citation.cfm?id=1553374.1553506.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.free_energies-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.free_energies","text":"free_energies(layer, inputs = 0; β = 1)\n\nCumulant generating function of units in layer (not reduced over layer dimensions).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.free_energy-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.free_energy","text":"free_energy(layer, inputs = 0; β = 1)\n\nCumulant generating function of layer, reduced over layer dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.free_energy-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.free_energy","text":"free_energy(rbm, v; β = 1)\n\nFree energy of visible configuration (after marginalizing hidden configurations).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.generate_sequences","page":"Reference","title":"RestrictedBoltzmannMachines.generate_sequences","text":"generate_sequences(n, A = 0:1)\n\nRetruns an iterator over all sequences of length n out of the alphabet A.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.initialize!","page":"Reference","title":"RestrictedBoltzmannMachines.initialize!","text":"initialize!(rbm, [data]; ϵ = 1e-6)\n\nInitializes the RBM parameters. If provided, matches average visible unit activities from data.\n\ninitialize!(layer, [data]; ϵ = 1e-6)\n\nInitializes a layer. If provided, matches average unit activities from data.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.initialize_weights!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.initialize_weights!","text":"initialize_weights!(rbm, data; λ = 0.1)\n\nInitializes RBM weights such that typical inputs to hidden units are λ.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.inputs_h_to_v-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.inputs_h_to_v","text":"inputs_h_to_v(rbm, h)\n\nInteraction inputs from hidden to visible layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.inputs_v_to_h-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.inputs_v_to_h","text":"inputs_v_to_h(rbm, v)\n\nInteraction inputs from visible to hidden layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.interaction_energy-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.interaction_energy","text":"interaction_energy(rbm, v, h)\n\nWeight mediated interaction energy.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_likelihood-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.log_likelihood","text":"log_likelihood(rbm, v; β = 1)\n\nLog-likelihood of v under rbm, with the partition function compued by extensive enumeration. For discrete layers, this is exponentially slow for large machines.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_partition-Tuple{RestrictedBoltzmannMachines.RBM}","page":"Reference","title":"RestrictedBoltzmannMachines.log_partition","text":"log_partition(rbm; β = 1)\n\nLog-partition of the rbm at inverse temperature β, computed by extensive enumeration of visible states (except for particular cases such as Gaussian-Gaussian RBM). This is exponentially slow for large machines.\n\nIf your RBM has a smaller hidden layer, mirroring the layers of the rbm first (see mirror).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_pseudolikelihood-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.log_pseudolikelihood","text":"log_pseudolikelihood(rbm, v; β = 1, exact = false)\n\nLog-pseudolikelihood of v. If exact is true, the exact pseudolikelihood is returned. But this is slow if v consists of many samples. Therefore by default exact is false, in which case the result is a stochastic approximation, where a random site is selected for each sample, and its conditional probability is calculated. In average the results with exact = false coincide with the deterministic result, and the estimate is more precise as the number of samples increases.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_pseudolikelihood_exact-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.log_pseudolikelihood_exact","text":"log_pseudolikelihood_exact(rbm, v; β = 1)\n\nLog-pseudolikelihood of v. This function computes the exact pseudolikelihood, doing traces over all sites. Note that this can be slow for large number of samples.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_pseudolikelihood_sites-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray, AbstractArray{<:CartesianIndex}}","page":"Reference","title":"RestrictedBoltzmannMachines.log_pseudolikelihood_sites","text":"log_pseudolikelihood_sites(rbm, v, sites; β = 1)\n\nLog-pseudolikelihood of a site conditioned on the other sites, where sites is an array of site indices (CartesianIndex), one for each sample. Returns an array of log-pseudolikelihood values, for each sample.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.log_pseudolikelihood_stoch-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.log_pseudolikelihood_stoch","text":"log_pseudolikelihood_stoch(rbm, v; β = 1)\n\nLog-pseudolikelihood of v. This function computes an stochastic approximation, by doing a trace over random sites for each sample. For large number of samples, this is in average close to the exact value of the pseudolikelihood.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.lognormcdf-Tuple{Real, Real}","page":"Reference","title":"RestrictedBoltzmannMachines.lognormcdf","text":"lognormcdf(a, b)\n\nComputes log(normcdf(a, b)), but retaining accuracy.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mean_-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.mean_","text":"mean_(A; dims)\n\nTakes the mean of A across dimensions dims and drops them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mean_h_from_v-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.mean_h_from_v","text":"mean_h_from_v(rbm, v; β = 1)\n\nMean unit activation values, conditioned on the other layer, <h | v>.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mean_v_from_h-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.mean_v_from_h","text":"mean_v_from_h(rbm, v; β = 1)\n\nMean unit activation values, conditioned on the other layer, <v | h>.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mills-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.mills","text":"mills(x::Real)\n\nMills ratio of the standard normal distribution. Defined as (1 - cdf(x)) / pdf(x).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.minibatch_count-Tuple{Int64}","page":"Reference","title":"RestrictedBoltzmannMachines.minibatch_count","text":"minibatch_count(nobs; batchsize)\n\nNumber of minibatches.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.minibatch_count-Tuple{Vararg{Union{Nothing, AbstractArray}}}","page":"Reference","title":"RestrictedBoltzmannMachines.minibatch_count","text":"minibatch_count(data; batchsize)\n\nNumber of minibatches.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.minibatches-Tuple{Int64}","page":"Reference","title":"RestrictedBoltzmannMachines.minibatches","text":"minibatches(nobs; batchsize, shuffle = true)\n\nPartition nobs into minibatches of length n. If necessary repeats some observations to complete last batches. (Therefore all batches are of the same size n).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.minibatches-Tuple{Vararg{Union{Nothing, AbstractArray}}}","page":"Reference","title":"RestrictedBoltzmannMachines.minibatches","text":"minibatches(datas...; batchsize)\n\nSplits the given datas into minibatches. Each minibatch is a tuple where each entry is a minibatch from the corresponding data within datas. All minibatches are of the same size batchsize (if necessary repeating some samples at the last minibatches).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mirror-Tuple{RestrictedBoltzmannMachines.RBM}","page":"Reference","title":"RestrictedBoltzmannMachines.mirror","text":"mirror(rbm)\n\nReturns a new RBM with viible and hidden layers flipped.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mode_h_from_v-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.mode_h_from_v","text":"mode_h_from_v(rbm, v)\n\nMode unit activations, conditioned on the other layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.mode_v_from_h-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.mode_v_from_h","text":"mode_v_from_h(rbm, h)\n\nMode unit activations, conditioned on the other layer.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.normcdf-Tuple{Real, Real}","page":"Reference","title":"RestrictedBoltzmannMachines.normcdf","text":"normcdf(a, b)\n\nProbablity that a ≤ Z ≤ b, where Z is a standard normal variable. WARNING: Silently returns a negative value if a > b.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.normcdf-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.normcdf","text":"normcdf(x)\n\nProbablity that Z ≤ x, where Z is a standard normal variable.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.normcdfinv-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.normcdfinv","text":"normcdfinv(x)\n\nInverse of normcdf.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.onehot_decode-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.onehot_decode","text":"onehot_decode(X)\n\nGiven a onehot encoded array X of N + 1 dimensions, returns the equivalent categorical array of N dimensions.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.onehot_encode","page":"Reference","title":"RestrictedBoltzmannMachines.onehot_encode","text":"onehot_encode(A, code)\n\nGiven an array A of N dimensions, returns a one-hot encoded BitArray of N + 1 dimensions where single entries of the first dimension are one.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.pcd!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.pcd!","text":"pcd!(rbm, data)\n\nTrains the RBM on data using Persistent Contrastive divergence.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.pcd!-Tuple{RestrictedBoltzmannMachines.RBM, RestrictedBoltzmannMachines.WeightNorm, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.pcd!","text":"pcd!(rbm, wn::WeightNorm, data)\n\nTrains the RBM on data, using the weight normalization heuristic. See Salimans & Kingma 2016, https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.pcd_bnorm!-Tuple{RestrictedBoltzmannMachines.RBM{<:RestrictedBoltzmannMachines.Binary, <:RestrictedBoltzmannMachines.Binary}, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.pcd_bnorm!","text":"pcd_bnorm!(rbm, data)\n\nTrains the RBM on data using Persistent Contrastive divergence, with centered gradients, as done in the PGM repo, https://github.com/jertubiana/PGM.\n\nThis is almost equivalent to centering visible units only (see https://github.com/cossio/CenteredRBMs.jl), but the centering of the hidden unit parameters is done much smoother.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.pgm_reg-Union{Tuple{RestrictedBoltzmannMachines.RBM{V}}, Tuple{V}} where V<:Union{RestrictedBoltzmannMachines.Binary, RestrictedBoltzmannMachines.Potts, RestrictedBoltzmannMachines.Spin}","page":"Reference","title":"RestrictedBoltzmannMachines.pgm_reg","text":"pgm_reg(rbm; λv, λw)\n\nRegularization used on https://github.com/jertubiana/PGM.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.randgumbel-Union{Tuple{}, Tuple{Type{T}}, Tuple{T}} where T","page":"Reference","title":"RestrictedBoltzmannMachines.randgumbel","text":"randgumbel(T = Float64)\n\nGenerates a random Gumbel variate.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.randnt-Tuple{Random.AbstractRNG, Real}","page":"Reference","title":"RestrictedBoltzmannMachines.randnt","text":"randnt([rng], a)\n\nRandom standard normal lower truncated at a (that is, Z ≥ a).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.randnt_half-Tuple{Random.AbstractRNG, Real, Real}","page":"Reference","title":"RestrictedBoltzmannMachines.randnt_half","text":"randnt_half([rng], μ, σ)\n\nSamples the normal distribution with mean μ and standard deviation σ truncated to positive values.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.rdm!-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.rdm!","text":"rdm!(rbm, data)\n\nTrains the RBM on data using contrastive divergence with randomly initialized chains. See http://arxiv.org/abs/2105.13889.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.reconstruction_error-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.reconstruction_error","text":"reconstruction_error(rbm, v; β = 1, steps = 1)\n\nStochastic reconstruction error of v.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sample_h_from_h-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.sample_h_from_h","text":"sample_h_from_h(rbm, h; β = 1, steps = 1)\n\nSamples a hidden configuration conditional on another hidden configuration h.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sample_h_from_v-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.sample_h_from_v","text":"sample_h_from_v(rbm, v; β = 1)\n\nSamples a hidden configuration conditional on the visible configuration v.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sample_v_from_h-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.sample_v_from_h","text":"sample_v_from_h(rbm, h; β = 1)\n\nSamples a visible configuration conditional on the hidden configuration h.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sample_v_from_v-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.sample_v_from_v","text":"sample_v_from_v(rbm, v; β = 1, steps = 1)\n\nSamples a visible configuration conditional on another visible configuration v.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sitedims-Tuple{RestrictedBoltzmannMachines.AbstractLayer}","page":"Reference","title":"RestrictedBoltzmannMachines.sitedims","text":"sitedims(layer)\n\nNumber of dimensions of layer, with special handling of Potts layer, for which the first dimension doesn't count as a site dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sitesize-Tuple{RestrictedBoltzmannMachines.AbstractLayer}","page":"Reference","title":"RestrictedBoltzmannMachines.sitesize","text":"sitesize(layer)\n\nSize of layer, with special handling of Potts layer, for which the first dimension doesn't count as a site dimension.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.sqrt1half-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.sqrt1half","text":"sqrt1half(x)\n\nAccurate computation of sqrt(1 + (x/2)^2) + |x|/2.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.std_-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.std_","text":"std_(A; dims)\n\nTakes the standard deviation of A across dimensions dims and drops them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.substitution_matrix_exhaustive","page":"Reference","title":"RestrictedBoltzmannMachines.substitution_matrix_exhaustive","text":"substitution_matrix_exhaustive(rbm, v; β = 1)\n\nReturns an q x N x B tensor of free energies F, where q is the number of possible values of each site, B the number of data points, and N the sequence length:\n\n`q, N, B = size(v)\n\nThus F and v have the same size. The entry F[x,i,b] gives the free energy cost of flipping site i to x of v[b] from its original value to x, that is:\n\nF[x,i,b] = free_energy(rbm, v_) - free_energy(rbm, v[b])\n\nwhere v_ is the same as v[b] in all sites but i, where v_ has the value x.\n\nNote that i can be a set of indices.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.substitution_matrix_sites","page":"Reference","title":"RestrictedBoltzmannMachines.substitution_matrix_sites","text":"substitution_matrix_sites(rbm, v, sites; β = 1)\n\nReturns an q x B matrix of free energies F, where q is the number of possible values of each site, and B the number of data points. The entry F[x,b] equals the free energy cost of flipping site[b] of v[b] to x, that is (schemetically):\n\nF[x, b] = free_energy(rbm, v_) - free_energy(rbm, v)\n\nwhere v = v[b], and v_ is the same as v in all sites except site[b], where v_ has the value x.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.sufficient_statistics","page":"Reference","title":"RestrictedBoltzmannMachines.sufficient_statistics","text":"sufficient_statistics(layer, data; [wts])\n\nReturns a NamedTuple of the sufficient statistics used by the layer.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.sum_-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.sum_","text":"sum_(A; dims)\n\nSums A over dimensions dims and drops them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.tnmean-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.tnmean","text":"tnmean(a)\n\nMean of the standard normal distribution, truncated to the interval (a, +∞).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.tnstd-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.tnstd","text":"tnstd(a)\n\nStandard deviation of the standard normal distribution, truncated to the interval (a, +∞).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.tnvar-Tuple{Real}","page":"Reference","title":"RestrictedBoltzmannMachines.tnvar","text":"tnvar(a)\n\nVariance of the standard normal distribution, truncated to the interval (a, +∞). WARNING: Fails for very very large values of a.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.train_white!-Tuple{RestrictedBoltzmannMachines.RBM{<:RestrictedBoltzmannMachines.Binary, <:RestrictedBoltzmannMachines.Binary}, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.train_white!","text":"train_white!(rbm, data)\n\nTrains the RBM on data.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_mean-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_mean","text":"transfer_mean(layer, inputs = 0; β = 1)\n\nMean of unit activations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_mean_abs-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_mean_abs","text":"transfer_mean_abs(layer, inputs = 0; β = 1)\n\nMean of absolute value of unit activations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_mode-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_mode","text":"transfer_mode(layer, inputs = 0)\n\nMode of unit activations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_sample-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_sample","text":"transfer_sample(layer, inputs = 0; β = 1)\n\nSamples layer configurations conditioned on inputs.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_std-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_std","text":"transfer_std(layer, inputs = 0; β = 1)\n\nStandard deviation of unit activations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.transfer_var-Tuple{RestrictedBoltzmannMachines.AbstractLayer, Union{Real, AbstractArray}}","page":"Reference","title":"RestrictedBoltzmannMachines.transfer_var","text":"transfer_var(layer, inputs = 0; β = 1)\n\nVariance of unit activations.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.var_-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.var_","text":"var_(A; dims)\n\nTakes the variance of A across dimensions dims and drops them.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.weight_norms-Tuple{RestrictedBoltzmannMachines.RBM}","page":"Reference","title":"RestrictedBoltzmannMachines.weight_norms","text":"weight_norms(rbm)\n\nNorms of weight patterns attached to each hidden unit.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.wmean-Tuple{AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.wmean","text":"wmean(A; wts = nothing, dims = :)\n\nWeighted mean of A along dimensions dims, with weights wts.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.zerosum!-Tuple{RestrictedBoltzmannMachines.RBM}","page":"Reference","title":"RestrictedBoltzmannMachines.zerosum!","text":"zerosum!(rbm)\n\nIf the rbm has Potts layers (visible or hidden), fixes zerosum gauge on the weights and on the layer fields. Otherwise, does nothing.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.∂energy","page":"Reference","title":"RestrictedBoltzmannMachines.∂energy","text":"∂energy(layer; stats...)\n\nDerivative of average energy of configurations with respect to layer parameters, where stats... refers to the sufficient statistics from samples required by the layer. See sufficient_statistics.\n\n\n\n\n\n","category":"function"},{"location":"reference/#RestrictedBoltzmannMachines.∂free_energy-Tuple{RestrictedBoltzmannMachines.AbstractLayer, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.∂free_energy","text":"∂free_energy(layer, inputs = 0; wts = 1)\n\nUnit activation moments, conjugate to layer parameters. These are obtained by differentiating free_energies with respect to the layer parameters. Averages over configurations (weigthed by wts).\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.∂free_energy-Tuple{RestrictedBoltzmannMachines.RBM, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.∂free_energy","text":"∂free_energy(rbm, v)\n\nGradient of free_energy(rbm, v) with respect to model parameters. If v consists of multiple samples (batches), then an average is taken.\n\n\n\n\n\n","category":"method"},{"location":"reference/#RestrictedBoltzmannMachines.∂wnorm-Tuple{AbstractArray, AbstractArray, AbstractArray}","page":"Reference","title":"RestrictedBoltzmannMachines.∂wnorm","text":"∂wnorm(∂w, g, v)\n\nGiven the gradients ∂w of a function f(w) with respect to w, returns the gradients ∂g, ∂v of f with respect to the re-parameterization:\n\nmathbfw = g fracmathbfvmathbfv\n\nSee Salimans & Kingma 2016, https://proceedings.neurips.cc/paper/2016/hash/ed265bc903a5a097f61d3ec064d96d2e-Abstract.html.\n\n\n\n\n\n","category":"method"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/mkl.jl\"","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"If you have an Intel CPU, then MKL is generally faster than OpenBLAS. Since multiplying by the RBM weights is one of the most time consuming operations in this package, it is advisable to use MKL. Let's do a quick comparison.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"To be sure we are using OpenBLAS first, we can run:","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"using LinearAlgebra\nLinearAlgebra.__init__() # don't need this on a fresh Julia session\nBLAS.get_config()","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Number of BLAS threads:","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"BLAS.get_num_threads()","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"We use Float32 for the test.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Float = Float32","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Now load MNIST","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"import MLDatasets\ntrain_x, train_y = MLDatasets.MNIST.traindata()\ntests_x, tests_y = MLDatasets.MNIST.testdata()\ntrain_x = Array{Float}(train_x .> 0.5)\ntests_x = Array{Float}(tests_x .> 0.5)\nnothing #hide","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"We train an RBM with plain contrastive divergence.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"import RestrictedBoltzmannMachines as RBMs\nrbm = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,128), zeros(Float,28,28,128))\nRBMs.initialize!(rbm, train_x)\nhistory_openblas = RBMs.cd!(rbm, train_x; epochs=50, batchsize=128, steps=1)\nnothing #hide","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Since we haven't loaded MKL, this first run should have used OpenBLAS. You can confirm that by doing:","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"using LinearAlgebra\nBLAS.get_config()","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"That should print that it's using libopenblas. Now load MKL. If you don't have it installed, run first pkg> add MKL.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"using MKL\nMKL.__init__() # don't need this on a fresh Julia session\nBLAS.get_config()","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Number of BLAS threads:","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"BLAS.get_num_threads()","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"In Julia 1.7 (which I assume you have), that's all you need to do. If you are on a fresh Julia session where MKL was not loaded before, you can skip the line MKL.__init__(). See https://julialang.org/blog/2021/11/julia-1.7-highlights/#libblastrampoline_mkljl. Now LinearAlgebra routines should forward to MKL (which should be confimed by the output of BLAS.get_config() above).","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"If for some reason you happen to want to go back to OpenBlAS, you can do LinearAlgebra.__init()__. And then MKL.__init__() to go back to MKL again, and so on.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"Now let's rerun the RBM training.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"rbm = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,128), zeros(Float,28,28,128))\nRBMs.initialize!(rbm, train_x)\nhistory_mkl = RBMs.cd!(rbm, train_x; epochs=50, batchsize=128, steps=1)\nnothing #hide","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"The epochs should be somewhat faster with MKL.","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"using CairoMakie\nfig = Figure(resolution=(600, 400))\nax = Axis(fig[1,1], xlabel=\"epoch\", ylabel=\"duration (seconds)\")\nlines!(ax, get(history_openblas, :Δt)..., label=\"OpenBLAS\")\nlines!(ax, get(history_mkl, :Δt)..., label=\"MKL\")\naxislegend(ax, position=:rt)\nfig","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"","category":"page"},{"location":"literate/mkl/","page":"MKL","title":"MKL","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#RestrictedBoltzmannMachines.jl-Documentation","page":"Home","title":"RestrictedBoltzmannMachines.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"A Julia package to train and simulate Restricted Boltzmann Machines. The package is registered. Install it with:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg\nPkg.add(\"RestrictedBoltzmannMachines\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"The source code is hosted on Github.","category":"page"},{"location":"","page":"Home","title":"Home","text":"https://github.com/cossio/RestrictedBoltzmannMachines.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package doesn't export any symbols. It can be imported like this:","category":"page"},{"location":"","page":"Home","title":"Home","text":"import RestrictedBoltzmannMachines as RBMs","category":"page"},{"location":"","page":"Home","title":"Home","text":"to avoid typing a long name everytime.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Most of the functions have a helpful docstring. See Reference section.","category":"page"},{"location":"","page":"Home","title":"Home","text":"See also the Examples listed on the menu on the left side bar to understand how the package works as a whole.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Training info is printed to the debug logger, and are hidden by default. To enable them, set:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ENV[\"JULIA_DEBUG\"] = RBMs","category":"page"},{"location":"","page":"Home","title":"Home","text":"See https://docs.julialang.org/en/v1/stdlib/Logging/ for more sophisticated approaches.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/MNIST/MNIST.jl\"","category":"page"},{"location":"literate/MNIST/MNIST/#MNIST","page":"MNIST","title":"MNIST","text":"","category":"section"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"We begin by importing the required packages. We load MNIST via the MLDatasets.jl package.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"using MKL # uses MKL for linear algebra (optional)\nnothing #hide\n\nusing CairoMakie, Statistics\nimport MLDatasets, Flux\nimport RestrictedBoltzmannMachines as RBMs\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Useful function to plot MNIST digits.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"\"\"\"\n    imggrid(A)\n\nGiven a four dimensional tensor `A` of size `(width, height, ncols, nrows)`\ncontaining `width x height` images in a grid of `nrows x ncols`, this returns\na matrix of size `(width * ncols, height * nrows)`, that can be plotted in a heatmap\nto display all images.\n\"\"\"\nfunction imggrid(A::AbstractArray{<:Any,4})\n    return reshape(permutedims(A, (1,3,2,4)), size(A,1)*size(A,3), size(A,2)*size(A,4))\nend","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Let's visualize some random digits.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\ndigits = MLDatasets.MNIST.traintensor()\ndigits = digits[:,:,rand(1:size(digits,3), nrows * ncols)]\ndigits = reshape(digits, 28, 28, ncols, nrows)\nimage!(ax, imggrid(digits), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Now load the full dataset.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"train_x, train_y = MLDatasets.MNIST.traindata()\ntests_x, tests_y = MLDatasets.MNIST.testdata()\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"train_x, tests_x contain the digit images, while train_y, tests_y contain the labels. We will train an RBM with binary (0,1) visible and hidden units. Therefore we binarize the data first. In addition, we restrict our attention to 0,1 digits only, so that training and so on are faster.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"selected_digits = (0, 1)\ntrain_x = train_x[:, :, train_y .∈ Ref(selected_digits)] .≥ 0.5\ntests_x = tests_x[:, :, tests_y .∈ Ref(selected_digits)] .≥ 0.5\ntrain_y = train_y[train_y .∈ Ref(selected_digits)]\ntests_y = tests_y[tests_y .∈ Ref(selected_digits)]\ntrain_nsamples = length(train_y)\ntests_nsamples = length(tests_y)\n(train_nsamples, tests_nsamples)","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"The above train_x and tests_x are BitArrays. Though convenient in terms of memory space, these are very slow in linear algebra. Since we frequently multiply data configurations times the weights of our RBM, we want to speed this up. So we convert to floats, which have much faster matrix multiplies thanks to BLAS. We will use Float32 here. To hit BLAS, this must be consistent with the types we use in the parameters of the RBM below.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Float = Float32\ntrain_x = Array{Float}(train_x)\ntests_x = Array{Float}(tests_x)\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Plot some examples of the binarized data.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\ndigits = reshape(train_x[:, :, rand(1:size(train_x,3), nrows * ncols)], 28, 28, ncols, nrows)\nimage!(ax, imggrid(digits), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Initialize an RBM with 100 hidden units. It is recommended to initialize the weights as random normals with zero mean and variance = 1/(number of visible units). See Glorot & Bengio 2010.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Notice how we pass the Float type, to set the parameter type of the layers and weights in the RBM.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"rbm = RBMs.RBM(RBMs.Binary(Float,28,28), RBMs.Binary(Float,200), randn(Float,28,28,200)/28)\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Initially, the RBM assigns a poor pseudolikelihood to the data.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, train_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, tests_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Incidentally, let us see how long it takes to evaluate the pseudolikelihood on the full dataset.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"@elapsed RBMs.log_pseudolikelihood(rbm, train_x) # pre-compiled by the calls above","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"This is the cost you pay when training by tracking the pseudolikelihood. The pseudolikelihood is computed on the full dataset every epoch. So if this time is too high compared to the computational time of training on an epoch, we should disable tracking the pseudolikelihood.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Now we train the RBM on the data. This returns a MVHistory object containing things like the pseudo-likelihood of the data during training. We print here the time spent in the training as a rough benchmark.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"@time history = RBMs.pcd!(\n    rbm, train_x; epochs=200, batchsize=256, optimizer=Flux.ADAM()\n)\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"After training, the pseudolikelihood score of the data improves significantly.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, train_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, tests_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Plot of log-pseudolikelihood during learning. Note that this shows the pseudolikelihood of the train data.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"lines(get(history, :lpl)...)","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Notice the abrupt oscillations at the beginning. Those come from the poor initialization of the RBM. We will correct this below.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Now let's generate some random RBM samples. First, we select random data digits to be initial conditions for the Gibbs sampling, and let's plot them.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"nrows, ncols = 10, 15\nfantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Now we do the Gibbs sampling to generate the RBM digits.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"@elapsed fantasy_x = RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Plot the resulting samples.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"fig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/#Parameter-initialization","page":"MNIST","title":"Parameter initialization","text":"","category":"section"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"If we initialize parameters, in particular matching the single-site statistics, the model trains better and faster.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"rbm = RBMs.RBM(\n    RBMs.Binary(Float,28,28),\n    RBMs.Binary(Float,200),\n    randn(Float,28,28,200)/28\n)\nRBMs.initialize!(rbm, train_x) # match single-site statistics\n@time history_init = RBMs.pcd!(\n    rbm, train_x; epochs=200, batchsize=256, optimizer=Flux.ADAM()\n)\nnothing #hide","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Compare the learning curves, with and without initialization.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"fig = Figure(resolution=(800, 300))\nax = Axis(fig[1,1])\nlines!(ax, get(history, :lpl)..., label=\"no init.\")\nlines!(ax, get(history_init, :lpl)..., label=\"init.\")\naxislegend(ax, position=:rb)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Notice how the pseudolikelihood curve grows a bit faster than before and is smoother. The initial oscillations are gone.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, train_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"RBMs.log_pseudolikelihood(rbm, tests_x) |> mean","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"Let's look at some samples generated by this RBM.","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"fantasy_x = train_x[:, :, rand(1:train_nsamples, nrows * ncols)]\nfantasy_x .= RBMs.sample_v_from_v(rbm, fantasy_x; steps=10000)\nfig = Figure(resolution=(40ncols, 40nrows))\nax = Axis(fig[1,1], yreversed=true)\nimage!(ax, imggrid(reshape(fantasy_x, 28, 28, ncols, nrows)), colorrange=(1,0))\nhidedecorations!(ax)\nhidespines!(ax)\nfig","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"","category":"page"},{"location":"literate/MNIST/MNIST/","page":"MNIST","title":"MNIST","text":"This page was generated using Literate.jl.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"EditURL = \"https://github.com/cossio/RestrictedBoltzmannMachines.jl/blob/master/docs/src/literate/layers/ReLU.jl\"","category":"page"},{"location":"literate/layers/ReLU/#ReLU-layer","page":"ReLU","title":"ReLU layer","text":"","category":"section"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"In this example we look at what the ReLU layer hidden units look like, for different parameter values.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"First load some packages.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"import RestrictedBoltzmannMachines as RBMs\nusing CairoMakie, Statistics\nnothing #hide","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"Now initialize our ReLU layer, with unit parameters spanning an interesting range.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"θs = [0; 10]\nγs = [5; 10]\nlayer = RBMs.ReLU([θ for θ in θs, γ in γs], [γ for θ in θs, γ in γs])\nnothing #hide","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"Now we sample our layer to collect some data.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"data = RBMs.transfer_sample(layer, zeros(size(layer)..., 10^6))\nnothing #hide","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"Let's plot the resulting histogram of the activations of each unit. We also overlay the analytical PDF.","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"fig = Figure(resolution=(700,500))\nax = Axis(fig[1,1])\nxs = repeat(reshape(range(minimum(data), maximum(data), 100), 1, 1, 100), size(layer)...)\nps = exp.(RBMs.free_energies(layer) .- RBMs.energies(layer, xs))\nfor (iθ, θ) in enumerate(θs), (iγ, γ) in enumerate(γs)\n    hist!(ax, data[iθ, iγ, :], normalization=:pdf, label=\"θ=$θ, γ=$γ\")\n    lines!(xs[iθ, iγ, :], ps[iθ, iγ, :], linewidth=2)\nend\naxislegend(ax)\nfig","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"","category":"page"},{"location":"literate/layers/ReLU/","page":"ReLU","title":"ReLU","text":"This page was generated using Literate.jl.","category":"page"}]
}
